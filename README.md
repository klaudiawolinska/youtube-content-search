# ğŸ“º YouTube Trends Explorer with Snowflake Cortex

## ğŸš€ Project Overview

This project collects trending YouTube videos + comments across multiple countries, ingests them into **Snowflake**, and powers a **Streamlit app (inside Snowsight)** that lets you **explore trends using natural language queries powered by Cortex Search**.

### Tech Stack

* **Python** â†’ extract data from the YouTube Data API
* **Docker** â†’ containerized data extraction for reproducibility
* **NDJSON** â†’ store raw videos and comments data
* **Snowflake SQL** â†’ ingestion
* **Snowflake Cortex** â†’ semantic search
* **Streamlit (in Snowsight)** â†’ interactive frontend

---

## ğŸ› ï¸ Pipeline Architecture

1. **Extraction (Python + Docker)**

   * Uses the YouTube API to fetch trending videos and top comments.
   * Data saved in batches as `.jsonl` files (newline-delimited JSON) files in `/output/`.
   * Supports multiple countries: ğŸ‡µğŸ‡± ğŸ‡ºğŸ‡¸ ğŸ‡¯ğŸ‡µ ğŸ‡³ğŸ‡¬ ğŸ‡§ğŸ‡· ğŸ‡®ğŸ‡³ ğŸ‡ªğŸ‡¬

2. **Ingestion (Snowflake SQL)**

   * Load `.jsonl` files into Snowflake with `COPY INTO`.
   * Create `videos` and `comments` tables.

3. **Cortex Integration**

   * Create a **Cortex Search Service** on top of comments and video metadata.
   * Enable **semantic queries** like:

     > â€œWhat are people saying about self-development videos in Japan?â€

4. **Streamlit App (Snowsight)**

   * Runs **inside Snowflake Snowsight** (no extra deployment needed).
   * Lets users search, filter, and explore trending content.
   * Uses Cortex to deliver natural-language semantic search.

---

## ğŸ“‚ Repository Structure

```
youtube-content-search/
â”œâ”€â”€ data/                    # Pre-collected sample data
â”‚   â”œâ”€â”€ youtube_trending_videos.jsonl
â”‚   â””â”€â”€ youtube_video_comments.jsonl
|
â”œâ”€â”€ extraction/              # Python ETL code
â”‚   â”œâ”€â”€ main.py              # entrypoint script
â”‚   â”œâ”€â”€ helpers.py           # API + JSON helpers
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile           
â”‚
â”œâ”€â”€ ingestion/               # Snowflake SQL script
â”‚   â””â”€â”€ data_ingestion.sql
â”‚
â”œâ”€â”€ app/                     # Streamlit app (runs in Snowsight)
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ“¸ Demo (Snowsight)

*(add screenshots or a short GIF of your app here)*

* Example search: *â€œTrending videos about AI in the USâ€*
* Example search: *â€œComments in Brazil related to musicâ€*

---

## âš¡ï¸ Quick Start

### Option A â€” Extract Fresh Data with Docker

Build the Docker image:

```bash
cd extraction
docker build -t youtube-extraction ./extraction
```

Run the extractor (replace `your_api_key_here` with your YouTube Data API key):

```bash
docker run -it --rm \
  -e YOUTUBE_API_KEY=your_api_key_here \
  -v $(pwd)/output:/app/output \
  youtube-extraction
```

This saves raw data into:

* `output/youtube_trending_videos.jsonl`
* `output/youtube_video_comments.jsonl`

### Option B â€” Use Sample Data (skip extraction)

If you donâ€™t want to set up the YouTube API, you can use the pre-collected sample data included in this repo:

- `data/youtube_trending_videos.jsonl`
- `data/youtube_video_comments.jsonl`

This lets you try the Snowflake + Cortex parts of the project right away.

### 2. Load into Snowflake

Run the `ingestion/data_ingestion.sql` SQL script inside **Snowsight Worksheets**.

### 3. Launch Streamlit in Snowsight

* Open Snowsight â†’ **Streamlit** â†’ **Create App**
* Paste in the code from `app/streamlit_app.py`
* Connect it to your Snowflake warehouse + database
* Start exploring with Cortex-powered search ğŸ‰
